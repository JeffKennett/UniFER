# UniFER 项目面试知识点总结

本文档总结了UniFER项目涉及的关键技术和知识点，帮助准备技术面试。

---

## 📚 一、深度学习基础

### 1.1 神经网络基本概念

#### 前向传播与反向传播
- **前向传播**: 输入数据通过网络层计算得到输出
- **反向传播**: 通过链式法则计算梯度，更新网络参数
- **梯度下降**: 沿着梯度负方向更新参数以最小化损失函数

#### 常见激活函数
- **ReLU**: `f(x) = max(0, x)` - 解决梯度消失，计算高效
- **GELU**: Gaussian Error Linear Unit - Transformer常用
- **Sigmoid**: 输出0-1，用于二分类
- **Softmax**: 多分类输出概率分布

### 1.2 优化器

#### AdamW
- Adam的改进版本，修正权重衰减实现
- UniFER使用AdamW优化器
- 超参数: learning_rate, beta1, beta2, weight_decay

#### 学习率调度
- **Cosine Annealing**: 余弦退火，学习率周期性变化
- **Warmup**: 开始阶段逐渐增加学习率，避免不稳定
- UniFER使用cosine scheduler

### 1.3 正则化技术

#### Dropout
- 训练时随机丢弃神经元，防止过拟合
- 测试时使用全部神经元

#### Gradient Clipping
- 限制梯度范数，防止梯度爆炸
- UniFER设置 `max_grad_norm=5`

---

## 🤖 二、Transformer架构

### 2.1 自注意力机制 (Self-Attention)

#### 计算公式
```
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

- **Q (Query)**: 查询向量
- **K (Key)**: 键向量  
- **V (Value)**: 值向量
- **√d_k**: 缩放因子，防止点积过大

#### 多头注意力 (Multi-Head Attention)
- 并行计算多组attention
- 捕获不同子空间的特征
- 最后concat并线性变换

### 2.2 Position Encoding
- **绝对位置编码**: sin/cos函数
- **相对位置编码**: 学习相对距离
- **RoPE**: 旋转位置编码，Qwen系列使用

### 2.3 层归一化 (Layer Normalization)
- 对每个样本的特征维度归一化
- 稳定训练过程
- 不同于Batch Normalization（对batch归一化）

---

## 🖼️ 三、视觉-语言多模态模型

### 3.1 多模态融合

#### 早期融合 (Early Fusion)
- 在输入层融合不同模态

#### 晚期融合 (Late Fusion)
- 各模态独立编码后融合

#### Qwen2.5-VL的方法
- 视觉token与文本token统一表示
- 通过cross-attention融合
- 端到端训练

### 3.2 视觉编码器

#### ViT (Vision Transformer)
- 图像切分为patches
- 每个patch作为一个token
- 使用Transformer编码

#### 图像预处理
- Resize到固定尺寸（224×224）
- Normalization: 减均值除标准差
- Data Augmentation: 增强数据多样性

### 3.3 生成式多模态模型

#### 核心思想
- 将视觉理解转化为文本生成任务
- 统一框架处理多种任务
- 利用大语言模型的推理能力

#### 优势
- 可解释性强（生成自然语言）
- 泛化能力好
- 可处理复杂推理任务

---

## 🔥 四、大语言模型 (LLM)

### 4.1 预训练

#### 自回归语言建模
- 给定前文预测下一个token
- 损失函数: Cross Entropy Loss
- 训练数据: 海量文本语料

#### Scaling Laws
- 模型越大，性能越好（在一定范围内）
- 参数量、数据量、计算量的权衡

### 4.2 提示学习 (Prompt Learning)

#### In-Context Learning
- 通过示例引导模型行为
- 无需参数更新

#### Chain-of-Thought (CoT)
- 让模型展示推理步骤
- 提升复杂推理能力
- UniFER的核心技术

#### Few-Shot Learning
- 少量样本快速适应新任务

### 4.3 解码策略

#### 贪婪解码 (Greedy Decoding)
- 每步选择概率最高的token
- 确定性输出
- UniFER推理时使用

#### 束搜索 (Beam Search)
- 保留top-k个候选序列
- 避免局部最优

#### 采样方法
- **Temperature Sampling**: 控制随机性
- **Top-p Sampling**: 从累积概率p的token中采样
- **Top-k Sampling**: 从概率最高的k个token中采样

---

## 🎓 五、微调技术

### 5.1 全参数微调 (Full Fine-Tuning)
- 更新模型所有参数
- 效果好但成本高
- 需要大量显存

### 5.2 参数高效微调 (PEFT)

#### LoRA (Low-Rank Adaptation)
- 冻结原始权重
- 添加低秩矩阵进行训练
- 大幅减少可训练参数
- UniFER可选使用

#### Adapter
- 在Transformer层插入小型网络
- 只训练adapter参数

#### Prefix Tuning
- 在输入添加可学习的前缀
- 冻结主模型参数

### 5.3 监督微调 (SFT)

#### 定义
- 使用标注数据进行有监督训练
- 适配特定下游任务

#### UniFER的SFT
- 数据: UniFER-CoT-230K
- 目标: 学习表情识别和CoT推理
- 输出格式: `<think>...</think><answer>...</answer>`

---

## 🏆 六、强化学习

### 6.1 基本概念

#### 马尔可夫决策过程 (MDP)
- **状态 (State)**: 环境的表示
- **动作 (Action)**: 智能体的选择
- **奖励 (Reward)**: 动作的即时反馈
- **策略 (Policy)**: 状态到动作的映射

#### 价值函数
- **V(s)**: 状态值函数
- **Q(s,a)**: 动作值函数

### 6.2 策略梯度方法

#### REINFORCE算法
- 直接优化策略网络
- 使用累积奖励作为梯度权重

#### PPO (Proximal Policy Optimization)
- 限制策略更新幅度
- 提升训练稳定性
- 广泛用于LLM对齐

### 6.3 GRPO (Group Relative Policy Optimization)

#### 核心思想
- 对每个prompt生成多个响应
- 计算相对奖励
- 优化组内排序

#### 优势
- 无需单独的奖励模型
- 更稳定的训练
- 适合文本生成任务

#### UniFER中的应用
- 生成8个候选响应
- 奖励函数: accuracy + format
- 使用vLLM加速生成

### 6.4 奖励函数设计

#### Accuracy Reward
- 检查答案是否正确
- 二值奖励: 0或1

#### Format Reward
- 检查输出格式
- 确保包含`<think>`和`<answer>`标签

#### 组合奖励
```python
total_reward = accuracy_reward + format_reward
```

---

## 💾 七、分布式训练

### 7.1 数据并行 (Data Parallelism)

#### DP (Data Parallel)
- 每个GPU复制完整模型
- 数据分片到各GPU
- 梯度同步后更新参数

#### DDP (Distributed Data Parallel)
- PyTorch推荐方式
- Ring-AllReduce通信
- 效率高于DP

### 7.2 DeepSpeed ZeRO

#### ZeRO-1
- 分片优化器状态
- 减少内存占用

#### ZeRO-2
- 分片优化器状态和梯度
- UniFER SFT阶段使用

#### ZeRO-3
- 分片模型参数、梯度、优化器状态
- 支持超大模型训练
- UniFER GRPO阶段使用

#### ZeRO-Offload
- CPU-GPU协同训练
- 进一步减少显存需求

### 7.3 混合精度训练

#### FP16 (Float16)
- 减半显存占用
- 加速计算
- 需要loss scaling防止下溢

#### BF16 (BFloat16)
- Google提出的格式
- 动态范围更大
- 无需loss scaling
- UniFER使用bf16

### 7.4 梯度累积 (Gradient Accumulation)

#### 原理
- 多个micro-batch累积梯度
- 一次更新参数
- 等效增大batch size

#### UniFER配置
- `gradient_accumulation_steps=4`
- 有效batch size = 1 × 4 × GPU数

---

## 📊 八、评估指标

### 8.1 分类指标

#### 准确率 (Accuracy)
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

#### 精确率 (Precision)
```
Precision = TP / (TP + FP)
```

#### 召回率 (Recall)
```
Recall = TP / (TP + FN)
```

#### F1-Score
```
F1 = 2 * Precision * Recall / (Precision + Recall)
```

### 8.2 多类别评估

#### 宏平均 (Macro Average)
- 各类别指标的算术平均
- 不考虑类别不平衡
- 适合类别平衡的数据

#### 微平均 (Micro Average)
- 先汇总所有类别的TP/FP/FN
- 再计算指标
- 受样本多的类别影响大

#### 加权平均 (Weighted Average)
- 按类别样本数加权
- 考虑类别不平衡

### 8.3 混淆矩阵

#### 定义
- 行: 真实标签
- 列: 预测标签
- 对角线: 正确预测

#### 分析
- 识别常见错误模式
- 找出易混淆的类别
- 指导模型改进

---

## 🛠️ 九、工程实践

### 9.1 代码组织

#### 模块化设计
- 数据处理、模型定义、训练、评估分离
- 便于维护和复用

#### 配置管理
- 使用配置文件或命令行参数
- 避免硬编码
- 易于实验管理

### 9.2 实验管理

#### Weights & Biases (wandb)
- 实时监控训练指标
- 可视化损失曲线
- 参数对比
- UniFER使用wandb记录

#### 检查点保存
- 定期保存模型
- 保留最佳模型
- 断点续训

### 9.3 性能优化

#### Flash Attention
- 优化attention计算
- 减少显存占用
- 提升训练速度
- UniFER启用flash_attention_2

#### Gradient Checkpointing
- 用计算换显存
- 重计算激活值而非存储
- 训练大模型必备

#### vLLM推理加速
- PagedAttention机制
- 高效KV缓存管理
- 批量推理优化
- GRPO阶段使用

---

## 📝 十、面试常见问题

### 10.1 项目相关

**Q: UniFER的主要创新点是什么？**

A: 
1. 提出FERBench基准评估MLLM在FER任务的表现
2. 构建CoT数据集和RLVR数据集
3. 两阶段训练范式: SFT + GRPO
4. 可解释的表情识别（生成推理过程）

**Q: 为什么使用思维链(CoT)？**

A:
1. 提升模型推理能力
2. 增强可解释性
3. 便于强化学习验证（格式化输出）
4. 符合人类认知过程

**Q: GRPO相比PPO的优势？**

A:
1. 无需单独的奖励模型和价值网络
2. 组内相对优化更稳定
3. 实现简单，易于调试
4. 适合生成任务

### 10.2 技术细节

**Q: 如何处理多模态融合？**

A:
1. 图像通过ViT编码为视觉token
2. 与文本token统一表示
3. 通过Transformer处理
4. 端到端训练

**Q: 如何防止过拟合？**

A:
1. Dropout正则化
2. 权重衰减 (weight_decay)
3. 数据增强
4. 早停 (Early Stopping)

**Q: 显存不足如何处理？**

A:
1. 减小batch size + 梯度累积
2. 使用DeepSpeed ZeRO-3
3. 启用gradient checkpointing
4. 混合精度训练 (bf16)
5. 参数高效微调 (LoRA)

### 10.3 算法理解

**Q: Attention机制的时间复杂度？**

A: O(n²d)，其中n是序列长度，d是隐藏维度。计算QK^T是O(n²d)。

**Q: Flash Attention如何优化？**

A: 
1. 分块计算，减少HBM访问
2. Kernel融合
3. 不存储完整attention矩阵
4. IO优化

**Q: 为什么Transformer需要位置编码？**

A: Self-attention是置换不变的，需要位置信息区分token顺序。

---

## 🎯 十一、简历描述建议

### 项目简述
```
UniFER: 基于多模态大语言模型的统一面部表情识别系统
- 构建了包含230K样本的思维链数据集和360K样本的强化学习数据集
- 提出两阶段训练范式：监督微调(SFT) + GRPO强化学习
- 在4个基准数据集上达到SOTA性能，超越Gemini-2.5-Pro等闭源模型
- 实现可解释的表情识别，生成详细的推理过程
```

### 技术关键词
- 多模态大语言模型 (MLLM)
- 视觉-语言预训练
- 思维链推理 (Chain-of-Thought)
- 强化学习 (GRPO)
- DeepSpeed分布式训练
- PyTorch, Transformers, TRL

### 职责描述
1. 设计并实现两阶段训练pipeline，包括监督微调和强化学习
2. 优化训练策略，使用DeepSpeed ZeRO-3和混合精度训练
3. 开发评估框架，在4个数据集上进行系统性评估
4. 实现可解释AI，通过思维链推理增强模型透明度

---

## 📚 推荐阅读

### 论文
1. **Attention Is All You Need** - Transformer原论文
2. **LoRA: Low-Rank Adaptation** - 参数高效微调
3. **Chain-of-Thought Prompting** - 思维链提示
4. **PPO: Proximal Policy Optimization** - 强化学习
5. **FlashAttention** - 高效attention实现

### 博客
- Hugging Face Blog
- OpenAI Research
- Google AI Blog

### 开源项目
- Transformers (Hugging Face)
- DeepSpeed (Microsoft)
- vLLM
- TRL (Transformer Reinforcement Learning)

---

这些知识点覆盖了UniFER项目的核心技术栈，建议逐一理解并能够结合项目经验进行阐述。
